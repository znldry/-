import json
import os
import sys
from typing import List, Dict, Any
import numpy as np
from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility
from sentence_transformers import SentenceTransformer

class MedicalVectorStore:
    """åŒ»ç–—RAGç³»ç»Ÿå‘é‡å­˜å‚¨ä¸æ£€ç´¢å™¨"""
    
    def __init__(self, 
                 collection_name: str = "medical_knowledge_base",
                 embedding_model_name: str = 'paraphrase-multilingual-MiniLM-L12-v2'):
        """
        åˆå§‹åŒ–å‘é‡å­˜å‚¨
        :param collection_name: Milvusé›†åˆåç§°
        :param embedding_model_name: ç”¨äºç¼–ç æŸ¥è¯¢çš„æ¨¡å‹ï¼ˆéœ€ä¸é¢„å¤„ç†æ—¶æ¨¡å‹ä¸€è‡´ï¼‰
        """
        self.collection_name = collection_name
        self.embedding_model = SentenceTransformer(embedding_model_name)
        self.dimension = self.embedding_model.get_sentence_embedding_dimension()
        
        print(f"[åˆå§‹åŒ–] è¿æ¥Milvuså¹¶å‡†å¤‡é›†åˆ: {collection_name}")
        print(f"[åˆå§‹åŒ–] åµŒå…¥æ¨¡å‹ç»´åº¦: {self.dimension}")
    
    def connect_to_milvus(self, host: str = "localhost", port: str = "19530"):
        """è¿æ¥åˆ°å·²å¯åŠ¨çš„ Milvus Standalone æœåŠ¡å™¨"""
        try:
            # ç›´æ¥è¿æ¥åˆ°æŒ‡å®šåœ°å€å’Œç«¯å£çš„ç‹¬ç«‹æœåŠ¡
            connections.connect("default", host=host, port=port)
            print(f"  âœ… å·²è¿æ¥åˆ° Milvus Standalone æœåŠ¡ ({host}:{port})ã€‚")
            return True
        except Exception as e:
            print(f"  âŒ è¿æ¥ Milvus å¤±è´¥: {e}")
            print("  æç¤º: è¯·ç¡®ä¿å·²æŒ‰ç…§æ­¥éª¤å¯åŠ¨ Docker å¹¶è¿è¡Œäº† `standalone.bat start` å‘½ä»¤ã€‚")
            return False
    
    def create_collection(self):
        """åˆ›å»ºMilvusé›†åˆï¼ˆæ•°æ®è¡¨ï¼‰"""
        # 1. å®šä¹‰å­—æ®µï¼ˆç±»ä¼¼æ•°æ®åº“è¡¨çš„åˆ—ï¼‰
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="chunk_id", dtype=DataType.INT64, description="åŸå§‹æ–‡æœ¬å—ID"),
            FieldSchema(name="chunk_text", dtype=DataType.VARCHAR, max_length=65535, description="æ–‡æœ¬å—å†…å®¹"),
            FieldSchema(name="text_length", dtype=DataType.INT64, description="æ–‡æœ¬é•¿åº¦"),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=self.dimension, description="æ–‡æœ¬å‘é‡"),
        ]
        
        # 2. åˆ›å»ºé›†åˆæ¨¡å¼
        schema = CollectionSchema(fields, description="åŒ»ç–—çŸ¥è¯†åº“å‘é‡å­˜å‚¨")
        
        # 3. åˆ›å»ºé›†åˆ
        self.collection = Collection(self.collection_name, schema, consistency_level="Strong")
        print(f"  âœ… é›†åˆ '{self.collection_name}' åˆ›å»ºæˆåŠŸã€‚")
        
        # 4. ä¸ºå‘é‡å­—æ®µåˆ›å»ºç´¢å¼•ï¼ˆåŠ é€Ÿæ£€ç´¢ï¼‰
        index_params = {
            "metric_type": "IP",  # å†…ç§¯ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
            "index_type": "IVF_FLAT",
            "params": {"nlist": 1024}  # èšç±»ä¸­å¿ƒæ•°ï¼Œå€¼è¶Šå¤§ç²¾åº¦è¶Šé«˜ï¼Œæ£€ç´¢ç¨æ…¢
        }
        self.collection.create_index("embedding", index_params)
        print(f"  âœ… å‘é‡ç´¢å¼•åˆ›å»ºæˆåŠŸ (ç±»å‹: IVF_FLAT, åº¦é‡: å†…ç§¯)ã€‚")
    
    def insert_chunks_from_file(self, chunks_file_path: str):
        """
        ä»é¢„å¤„ç†å¥½çš„JSONæ–‡ä»¶è¯»å–æ–‡æœ¬å—å’Œå‘é‡ï¼Œå¹¶æ’å…¥Milvus
        """
        print(f"[æ•°æ®æ’å…¥] ä»æ–‡ä»¶åŠ è½½å—æ•°æ®: {chunks_file_path}")
        
        if not os.path.exists(chunks_file_path):
            print(f"  âŒ æ–‡ä»¶ä¸å­˜åœ¨: {chunks_file_path}")
            print(f"  è¯·å…ˆè¿è¡Œ `python src/preprocessor.py` ç”Ÿæˆæ•°æ®ã€‚")
            return False
        
        try:
            with open(chunks_file_path, 'r', encoding='utf-8') as f:
                chunks_data = json.load(f)
            
            print(f"  âœ… æˆåŠŸåŠ è½½ {len(chunks_data)} ä¸ªæ–‡æœ¬å—ã€‚")
            
            # å‡†å¤‡æ‰¹é‡æ’å…¥çš„æ•°æ®åˆ—è¡¨
            chunk_ids = []
            chunk_texts = []
            text_lengths = []
            embeddings = []
            
            for chunk in chunks_data:
                chunk_ids.append(chunk['chunk_id'])
                chunk_texts.append(chunk['text'])
                text_lengths.append(chunk['length'])
                # ç¡®ä¿åµŒå…¥å‘é‡æ˜¯åˆ—è¡¨æ ¼å¼
                if 'embedding' in chunk:
                    if isinstance(chunk['embedding'], list):
                        embeddings.append(chunk['embedding'])
                    else:
                        # å¦‚æœæ˜¯numpyæ•°ç»„ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
                        embeddings.append(chunk['embedding'].tolist() if hasattr(chunk['embedding'], 'tolist') else list(chunk['embedding']))
                else:
                    print(f"  âš ï¸  å— {chunk['chunk_id']} ç¼ºå°‘åµŒå…¥å‘é‡ï¼Œå°†è·³è¿‡ã€‚")
            
            # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
            if len(embeddings) != len(chunks_data):
                print("  âŒ éƒ¨åˆ†å—ç¼ºå°‘åµŒå…¥å‘é‡ï¼Œæ’å…¥ç»ˆæ­¢ã€‚")
                return False
            
            # æ„å»ºæ’å…¥æ•°æ®
            entities = [
                chunk_ids,
                chunk_texts,
                text_lengths,
                embeddings
            ]
            
            # æ‰§è¡Œæ’å…¥
            print(f"  æ­£åœ¨æ’å…¥ {len(chunk_ids)} æ¡è®°å½•åˆ°Milvus...")
            insert_result = self.collection.insert(entities)
            
            # æ’å…¥åï¼Œéœ€è¦å°†æ•°æ®ä»å†…å­˜å†™å…¥ç£ç›˜ï¼ˆåˆ·æ–°ï¼‰
            self.collection.flush()
            
            print(f"  âœ… æ•°æ®æ’å…¥æˆåŠŸï¼æ’å…¥æ•°é‡: {insert_result.insert_count}")
            print(f"  ğŸ’¾ æ•°æ®å·²æŒä¹…åŒ–ã€‚")
            
            # åŠ è½½é›†åˆåˆ°å†…å­˜ï¼ˆä½¿æ•°æ®å¯è¢«æ£€ç´¢ï¼‰
            self.collection.load()
            print(f"  âœ… é›†åˆå·²åŠ è½½åˆ°å†…å­˜ï¼Œå‡†å¤‡å°±ç»ªã€‚")
            
            return True
            
        except Exception as e:
            print(f"  âŒ æ’å…¥æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def search_similar_chunks(self, query_text: str, top_k: int = 5):
        """
        æ£€ç´¢ä¸æŸ¥è¯¢æœ€ç›¸ä¼¼çš„æ–‡æœ¬å—
        :param query_text: ç”¨æˆ·æŸ¥è¯¢ï¼ˆé—®é¢˜ï¼‰
        :param top_k: è¿”å›æœ€ç›¸ä¼¼çš„ç»“æœæ•°é‡
        :return: æ£€ç´¢åˆ°çš„æ–‡æœ¬å—åˆ—è¡¨
        """
        if not hasattr(self, 'collection') or self.collection.is_empty:
            print("  âš ï¸  é›†åˆä¸ºç©ºæˆ–æœªåŠ è½½ï¼Œæ— æ³•æ£€ç´¢ã€‚")
            return []
        
        # 1. å°†æŸ¥è¯¢æ–‡æœ¬ç¼–ç ä¸ºå‘é‡
        print(f"  æ­£åœ¨ç¼–ç æŸ¥è¯¢: \"{query_text[:50]}...\"")
        query_embedding = self.embedding_model.encode([query_text])
        
        # 2. å‡†å¤‡æœç´¢å‚æ•°
        search_params = {
            "metric_type": "IP",
            "params": {"nprobe": 64}  # æœç´¢çš„èšç±»ä¸­å¿ƒæ•°ï¼Œå€¼è¶Šå¤§ç²¾åº¦è¶Šé«˜ï¼Œæ£€ç´¢ç¨æ…¢
        }
        
        # 3. æ‰§è¡Œå‘é‡ç›¸ä¼¼æ€§æœç´¢
        results = self.collection.search(
            data=query_embedding,  # æŸ¥è¯¢å‘é‡
            anns_field="embedding",  # æœç´¢çš„å‘é‡å­—æ®µ
            param=search_params,
            limit=top_k,  # è¿”å›top_kä¸ªç»“æœ
            output_fields=["chunk_id", "chunk_text", "text_length"]  # åŒæ—¶è¿”å›è¿™äº›å­—æ®µ
        )
        
        # 4. è§£æå¹¶æ ¼å¼åŒ–ç»“æœ
        retrieved_chunks = []
        if results:
            for hits in results:
                for hit in hits:
                    chunk_info = {
                        'chunk_id': hit.entity.get('chunk_id'),
                        'text': hit.entity.get('chunk_text'),
                        'length': hit.entity.get('text_length'),
                        'similarity_score': hit.score,  # ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå†…ç§¯å€¼ï¼Œè¶Šå¤§è¶Šç›¸ä¼¼ï¼‰
                        'distance': 1 - hit.score  # è½¬æ¢ä¸ºè·ç¦»ï¼ˆä½™å¼¦è·ç¦»ï¼‰
                    }
                    retrieved_chunks.append(chunk_info)
        
        return retrieved_chunks
    
    def display_search_results(self, query: str, results: List[Dict], top_k: int = 3):
        """ç¾è§‚åœ°å±•ç¤ºæ£€ç´¢ç»“æœ"""
        print(f"\nğŸ” æŸ¥è¯¢: \"{query}\"")
        print(f"ğŸ“Š è¿”å› {len(results)} ä¸ªæœ€ç›¸å…³ç»“æœ (æ˜¾ç¤ºå‰{top_k}ä¸ª):")
        print("-" * 80)
        
        for i, chunk in enumerate(results[:top_k]):
            print(f"\nğŸ† ç»“æœ #{i+1} (ç›¸ä¼¼åº¦: {chunk['similarity_score']:.4f})")
            print(f"   æ–‡æœ¬å—ID: {chunk['chunk_id']} | é•¿åº¦: {chunk['length']} å­—ç¬¦")
            print(f"   ğŸ“„ å†…å®¹é¢„è§ˆ: {chunk['text'][:150]}...")
            print("-" * 60)
    
    def test_with_sample_questions(self, questions_file_path: str, num_test_questions: int = 3):
        """
        ä½¿ç”¨é—®é¢˜æ–‡ä»¶ä¸­çš„æ ·æœ¬æ¥æµ‹è¯•æ£€ç´¢ç³»ç»Ÿ
        """
        print(f"\n[é›†æˆæµ‹è¯•] ä½¿ç”¨é—®é¢˜æ–‡ä»¶æµ‹è¯•æ£€ç´¢: {questions_file_path}")
        
        if not os.path.exists(questions_file_path):
            print(f"  âŒ é—®é¢˜æ–‡ä»¶ä¸å­˜åœ¨: {questions_file_path}")
            return
        
        try:
            with open(questions_file_path, 'r', encoding='utf-8') as f:
                questions_data = json.load(f)
            
            # ç¡®ä¿æ˜¯åˆ—è¡¨æ ¼å¼
            if isinstance(questions_data, list):
                test_questions = questions_data[:num_test_questions]
            else:
                print("  âŒ é—®é¢˜æ–‡ä»¶æ ¼å¼ä¸æ˜¯åˆ—è¡¨ï¼Œæ— æ³•æµ‹è¯•ã€‚")
                return
            
            print(f"  âœ… åŠ è½½ {len(test_questions)} ä¸ªæµ‹è¯•é—®é¢˜ã€‚")
            
            for i, qa_pair in enumerate(test_questions):
                question = qa_pair.get('question', '')
                if question:
                    print(f"\n{'='*80}")
                    print(f"æµ‹è¯• #{i+1}")
                    # æ‰§è¡Œæ£€ç´¢
                    results = self.search_similar_chunks(question, top_k=5)
                    # æ˜¾ç¤ºç»“æœ
                    self.display_search_results(question, results, top_k=2)
                    
                    # æ˜¾ç¤ºå‚è€ƒç­”æ¡ˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if 'answer' in qa_pair:
                        print(f"\nğŸ’¡ å‚è€ƒç­”æ¡ˆ: {qa_pair['answer'][:200]}...")
            
            print(f"\n{'='*80}")
            print("âœ… æ£€ç´¢æµ‹è¯•å®Œæˆï¼")
            
        except Exception as e:
            print(f"  âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°ï¼šå®Œæ•´çš„å‘é‡å­˜å‚¨ä¸æ£€ç´¢æµæ°´çº¿"""
    print("=" * 80)
    print("åŒ»ç–—RAGç³»ç»Ÿ - å‘é‡å­˜å‚¨ä¸æ£€ç´¢æ¨¡å—")
    print("=" * 80)
    
    # 1. å®šä¹‰è·¯å¾„ (æ ¹æ®ä½ çš„å®é™…é¡¹ç›®è·¯å¾„è°ƒæ•´)
    BASE_DIR = "D:/lesson/exp4/GraphRAG-Benchmark-main"
    CHUNKS_FILE = os.path.join(BASE_DIR, "processed_data", "medical_chunks.json").replace('\\', '/')
    QUESTIONS_FILE = os.path.join(BASE_DIR, "Data", "Questions", "medical_questions.json").replace('\\', '/')
    
    # 2. åˆå§‹åŒ–å‘é‡å­˜å‚¨
    print("\n[é˜¶æ®µ1] åˆå§‹åŒ–å‘é‡å­˜å‚¨ç³»ç»Ÿ")
    vector_store = MedicalVectorStore(collection_name="medical_knowledge_v1")
    
    # 3. è¿æ¥åˆ°Milvus
    if not vector_store.connect_to_milvus():
        print("âŒ æ— æ³•è¿æ¥Milvusï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯åŠ¨ã€‚")
        print("   å¯åŠ¨å‘½ä»¤: `milvus-server` (åœ¨ç‹¬ç«‹ç»ˆç«¯ä¸­è¿è¡Œ)")
        return
    
    # 4. åˆ›å»ºé›†åˆï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if not utility.has_collection(vector_store.collection_name):
        vector_store.create_collection()
    else:
        print(f"\n[ä¿¡æ¯] é›†åˆ '{vector_store.collection_name}' å·²å­˜åœ¨ã€‚")
        vector_store.collection = Collection(vector_store.collection_name)
        # ç¡®ä¿é›†åˆå·²åŠ è½½
        vector_store.collection.load()
    
    # 5. æ£€æŸ¥å¹¶æ’å…¥æ•°æ®
    print(f"\n[é˜¶æ®µ2] å‡†å¤‡å‘é‡æ•°æ®")
    # æ£€æŸ¥é›†åˆä¸­æ˜¯å¦å·²æœ‰æ•°æ®
    if vector_store.collection.is_empty:
        print("  é›†åˆä¸ºç©ºï¼Œå¼€å§‹æ’å…¥æ•°æ®...")
        success = vector_store.insert_chunks_from_file(CHUNKS_FILE)
        if not success:
            print("âŒ æ•°æ®æ’å…¥å¤±è´¥ï¼Œæµç¨‹ç»ˆæ­¢ã€‚")
            return
    else:
        entity_count = vector_store.collection.num_entities
        print(f"  é›†åˆä¸­å·²æœ‰ {entity_count} æ¡æ•°æ®ï¼Œè·³è¿‡æ’å…¥ã€‚")
    
    # 6. è¿›è¡Œé›†æˆæµ‹è¯•
    print(f"\n[é˜¶æ®µ3] æ£€ç´¢åŠŸèƒ½é›†æˆæµ‹è¯•")
    vector_store.test_with_sample_questions(QUESTIONS_FILE, num_test_questions=3)
    
    # 7. äº¤äº’å¼æŸ¥è¯¢æ¼”ç¤º
    print(f"\n[é˜¶æ®µ4] äº¤äº’å¼æŸ¥è¯¢æ¼”ç¤º (è¾“å…¥ 'quit' é€€å‡º)")
    print("-" * 80)
    
    while True:
        try:
            user_query = input("\nè¯·è¾“å…¥åŒ»ç–—é—®é¢˜ (æˆ–è¾“å…¥ 'quit' é€€å‡º): ").strip()
            
            if user_query.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ é€€å‡ºäº¤äº’å¼æŸ¥è¯¢ã€‚")
                break
            
            if not user_query:
                continue
            
            # æ‰§è¡Œæ£€ç´¢
            results = vector_store.search_similar_chunks(user_query, top_k=4)
            
            if results:
                vector_store.display_search_results(user_query, results, top_k=3)
            else:
                print("  æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç¨‹åºè¢«ä¸­æ–­ã€‚")
            break
        except Exception as e:
            print(f"  æ£€ç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {e}")

if __name__ == "__main__":
    # Windowsæ§åˆ¶å°ç¼–ç è®¾ç½®
    if sys.platform.startswith('win'):
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    
    main()