import json
import re
import os
import sys
from typing import List, Dict, Any, Tuple
import numpy as np
from sentence_transformers import SentenceTransformer

class MedicalTextProcessor:
    """åŒ»ç–—æ–‡æœ¬å¤„ç†å™¨ï¼šè´Ÿè´£æ¸…æ´—ã€åˆ†å—å’Œå‘é‡åŒ–"""
    
    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2'):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        :param model_name: åµŒå…¥æ¨¡å‹åç§°ï¼Œæ¨èçš„å¤šè¯­è¨€æ¨¡å‹
        """
        print(f"[åˆå§‹åŒ–] åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}")
        # æ­¤æ¨¡å‹æ”¯æŒä¸­è‹±æ–‡ï¼Œé€‚åˆä½ çš„å®éªŒè¦æ±‚
        self.embedding_model = SentenceTransformer(model_name)
        self.model_dimension = self.embedding_model.get_sentence_embedding_dimension()
        print(f"[åˆå§‹åŒ–] æ¨¡å‹ç»´åº¦: {self.model_dimension}")
        
    def load_and_extract_text(self, corpus_path: str) -> str:
        """
        åŠ è½½åŸå§‹æ•°æ®å¹¶æå–æ ¸å¿ƒæ–‡æœ¬å†…å®¹
        :return: æå–å‡ºçš„çº¯æ–‡æœ¬å­—ç¬¦ä¸²
        """
        print(f"[æ­¥éª¤1] ä»æ–‡ä»¶ä¸­æå–æ ¸å¿ƒæ–‡æœ¬: {corpus_path}")
        try:
            with open(corpus_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, dict) and 'context' in data:
                main_text = data['context']
                print(f"  âœ… æˆåŠŸæå– 'context' é”®ä¸‹çš„æ–‡æœ¬")
                print(f"  ğŸ“Š æ–‡æœ¬æ€»é•¿åº¦: {len(main_text)} å­—ç¬¦")
                # æ˜¾ç¤ºå¼€å¤´å’Œç»“å°¾éƒ¨åˆ†
                print(f"  ğŸ” æ–‡æœ¬å¼€å¤´: {main_text[:100]}...")
                print(f"  ğŸ” æ–‡æœ¬ç»“å°¾: ...{main_text[-100:]}")
                return main_text
            else:
                print("  âŒ é”™è¯¯ï¼šæ•°æ®å­—å…¸ä¸­æœªæ‰¾åˆ° 'context' é”®")
                return ""
                
        except Exception as e:
            print(f"  âŒ åŠ è½½æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return ""
    
    def clean_text(self, text: str) -> str:
        """
        æ¸…æ´—æ–‡æœ¬ï¼šç§»é™¤å¤šä½™ç©ºæ ¼ã€æ¢è¡Œï¼Œè§„èŒƒåŒ–æ ¼å¼
        """
        print("[æ­¥éª¤2] æ¸…æ´—æ–‡æœ¬...")
        # åˆå¹¶å¤šä¸ªæ¢è¡Œç¬¦å’Œç©ºæ ¼
        text = re.sub(r'\n+', '\n', text)
        text = re.sub(r'[ \t]+', ' ', text)
        # ç¡®ä¿æ®µè½ä¹‹é—´æœ‰é€‚å½“çš„ç©ºæ ¼
        text = re.sub(r'\n', ' ', text)
        text = re.sub(r'\.\s+', '.\n', text)  # åœ¨å¥å·åé‡æ–°æ·»åŠ æ¢è¡Œï¼Œä¾¿äºè¯†åˆ«å¥å­
        print(f"  âœ… æ–‡æœ¬æ¸…æ´—å®Œæˆ")
        return text
    
    def split_into_paragraphs(self, text: str, min_paragraph_length: int = 50) -> List[str]:
        """
        å°†é•¿æ–‡æœ¬æŒ‰æ®µè½åˆ†å‰²ï¼ˆåŸºäºæ¢è¡Œæˆ–å¥å­è¾¹ç•Œï¼‰
        """
        print("[æ­¥éª¤3] å°†æ–‡æœ¬åˆ†å‰²ä¸ºåˆå§‹æ®µè½...")
        # å…ˆæŒ‰æ¢è¡Œåˆ†å‰²
        raw_paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
        
        # è¿‡æ»¤æ‰è¿‡çŸ­çš„æ®µè½ï¼ˆå¯èƒ½æ˜¯æ ‡é¢˜æˆ–ç¼–å·ï¼‰
        paragraphs = [p for p in raw_paragraphs if len(p) >= min_paragraph_length]
        
        print(f"  ğŸ“Š è·å¾— {len(paragraphs)} ä¸ªæ½œåœ¨æ®µè½")
        if paragraphs:
            print(f"  ğŸ” æ ·ä¾‹æ®µè½ (é•¿åº¦: {len(paragraphs[0])}): {paragraphs[0][:100]}...")
        return paragraphs
    
    def chunk_paragraphs(self, paragraphs: List[str], 
                         max_chunk_size: int = 500, 
                         overlap: int = 50) -> List[Dict[str, Any]]:
        """
        æ™ºèƒ½åˆ†å—ï¼šå°†æ®µè½åˆå¹¶ä¸ºé€‚å½“å¤§å°çš„æ–‡æœ¬å—ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´
        """
        print(f"[æ­¥éª¤4] æ™ºèƒ½åˆ†å— (ç›®æ ‡å¤§å°: {max_chunk_size}å­—ç¬¦, é‡å : {overlap}å­—ç¬¦)...")
        
        chunks = []
        current_chunk = ""
        current_length = 0
        chunk_id = 0
        
        for i, para in enumerate(paragraphs):
            para_length = len(para)
            
            # å¦‚æœå½“å‰æ®µè½æœ¬èº«å°±å¾ˆé•¿ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
            if para_length > max_chunk_size:
                # å¦‚æœå½“å‰å—æœ‰å†…å®¹ï¼Œå…ˆä¿å­˜
                if current_chunk:
                    chunks.append({
                        'chunk_id': chunk_id,
                        'text': current_chunk.strip(),
                        'length': len(current_chunk.strip()),
                        'source_paragraphs': f'{i}'
                    })
                    chunk_id += 1
                    current_chunk = ""
                    current_length = 0
                
                # å¯¹é•¿æ®µè½æŒ‰å¥å­åˆ†å‰²
                sentences = re.split(r'(?<=[.!?])\s+', para)
                sub_chunk = ""
                for sent in sentences:
                    sent_length = len(sent)
                    if len(sub_chunk) + sent_length <= max_chunk_size:
                        sub_chunk += " " + sent if sub_chunk else sent
                    else:
                        if sub_chunk:
                            chunks.append({
                                'chunk_id': chunk_id,
                                'text': sub_chunk.strip(),
                                'length': len(sub_chunk.strip()),
                                'source_paragraphs': f'{i}(éƒ¨åˆ†)'
                            })
                            chunk_id += 1
                        sub_chunk = sent
                
                # å¤„ç†å‰©ä½™éƒ¨åˆ†
                if sub_chunk:
                    current_chunk = sub_chunk
                    current_length = len(sub_chunk)
            
            # å¦‚æœæ®µè½å¯ä»¥åŠ å…¥å½“å‰å—
            elif current_length + para_length <= max_chunk_size:
                current_chunk += " " + para if current_chunk else para
                current_length += para_length
            
            # å¦‚æœæ®µè½å¤ªå¤§ï¼Œç»“æŸå½“å‰å—å¹¶å¼€å§‹æ–°å—
            else:
                if current_chunk:
                    chunks.append({
                        'chunk_id': chunk_id,
                        'text': current_chunk.strip(),
                        'length': len(current_chunk.strip()),
                        'source_paragraphs': f'{i-1 if i>0 else i}'
                    })
                    chunk_id += 1
                
                # æ–°å—ä»å½“å‰æ®µè½å¼€å§‹ï¼Œå¹¶åŒ…å«ä¸€äº›é‡å 
                if overlap > 0 and current_chunk:
                    # ä»ä¸Šä¸€å—å–æœ«å°¾éƒ¨åˆ†ä½œä¸ºé‡å 
                    overlap_text = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk
                    current_chunk = overlap_text + " " + para
                else:
                    current_chunk = para
                current_length = len(current_chunk)
        
        # å¤„ç†æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunks.append({
                'chunk_id': chunk_id,
                'text': current_chunk.strip(),
                'length': len(current_chunk.strip()),
                'source_paragraphs': 'final'
            })
        
        print(f"  âœ… ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
        print(f"  ğŸ“Š å—å¤§å°ç»Ÿè®¡:")
        if chunks:
            lengths = [c['length'] for c in chunks]
            print(f"    æœ€å°: {min(lengths)} å­—ç¬¦, æœ€å¤§: {max(lengths)} å­—ç¬¦, å¹³å‡: {sum(lengths)/len(lengths):.1f} å­—ç¬¦")
        
        # æ˜¾ç¤ºå‰3ä¸ªå—ä½œä¸ºæ ·ä¾‹
        for i, chunk in enumerate(chunks[:3]):
            print(f"  ğŸ” å—{i+1} (é•¿åº¦:{chunk['length']}): {chunk['text'][:80]}...")
        
        return chunks
    
    def generate_embeddings(self, chunks: List[Dict[str, Any]]) -> Tuple[List[np.ndarray], List[Dict[str, Any]]]:
        """
        ä¸ºæ‰€æœ‰æ–‡æœ¬å—ç”ŸæˆåµŒå…¥å‘é‡
        """
        print(f"[æ­¥éª¤5] ä¸º {len(chunks)} ä¸ªæ–‡æœ¬å—ç”ŸæˆåµŒå…¥å‘é‡...")
        
        texts = [chunk['text'] for chunk in chunks]
        
        # æ‰¹é‡ç”ŸæˆåµŒå…¥ï¼ˆæ˜¾ç¤ºè¿›åº¦æ¡ï¼‰
        print("  æ­£åœ¨ç¼–ç ... (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)")
        embeddings = self.embedding_model.encode(texts, show_progress_bar=True)
        
        print(f"  âœ… åµŒå…¥ç”Ÿæˆå®Œæˆ")
        print(f"  ğŸ“Š åµŒå…¥å½¢çŠ¶: {embeddings.shape}")
        
        # æ›´æ–°å—ä¿¡æ¯ï¼Œæ·»åŠ åµŒå…¥å‘é‡
        for i, chunk in enumerate(chunks):
            chunk['embedding'] = embeddings[i]
        
        return embeddings, chunks
    
    def save_chunks_to_json(self, chunks: List[Dict[str, Any]], output_path: str):
        """å°†å¤„ç†åçš„å—ä¿å­˜ä¸ºJSONæ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•å’Œæ£€æŸ¥ï¼‰"""
        # æ³¨æ„ï¼šåµŒå…¥å‘é‡å¾ˆå¤§ï¼Œä¿å­˜æ—¶è½¬æ¢ä¸ºåˆ—è¡¨
        save_data = []
        for chunk in chunks:
            chunk_copy = chunk.copy()
            if 'embedding' in chunk_copy:
                chunk_copy['embedding'] = chunk_copy['embedding'].tolist() if hasattr(chunk_copy['embedding'], 'tolist') else list(chunk_copy['embedding'])
            save_data.append(chunk_copy)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        print(f"  ğŸ’¾ å—æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
    
    def process_pipeline(self, corpus_path: str, output_dir: str = "processed_data") -> List[Dict[str, Any]]:
        """
        å®Œæ•´çš„å¤„ç†æµæ°´çº¿
        """
        print("=" * 60)
        print("å¼€å§‹åŒ»ç–—æ–‡æœ¬å¤„ç†æµæ°´çº¿")
        print("=" * 60)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. åŠ è½½å¹¶æå–æ–‡æœ¬
        raw_text = self.load_and_extract_text(corpus_path)
        if not raw_text:
            print("âŒ æ— æ³•æå–æ–‡æœ¬ï¼Œæµæ°´çº¿ç»ˆæ­¢")
            return []
        
        # 2. æ¸…æ´—æ–‡æœ¬
        cleaned_text = self.clean_text(raw_text)
        
        # 3. åˆ†å‰²æ®µè½
        paragraphs = self.split_into_paragraphs(cleaned_text)
        
        # 4. æ™ºèƒ½åˆ†å—
        chunks = self.chunk_paragraphs(paragraphs, max_chunk_size=600, overlap=80)
        
        # 5. ç”ŸæˆåµŒå…¥å‘é‡
        embeddings, chunks_with_embeddings = self.generate_embeddings(chunks)
        
        # 6. ä¿å­˜ç»“æœ
        output_path = os.path.join(output_dir, "medical_chunks.json")
        self.save_chunks_to_json(chunks_with_embeddings, output_path)
        
        print("=" * 60)
        print("âœ… æ–‡æœ¬å¤„ç†æµæ°´çº¿å®Œæˆï¼")
        print(f"   ç”Ÿæˆ {len(chunks_with_embeddings)} ä¸ªæ–‡æœ¬å—")
        print(f"   åµŒå…¥ç»´åº¦: {self.model_dimension}")
        print("=" * 60)
        
        return chunks_with_embeddings

def main():
    """ä¸»å‡½æ•°"""
    
    # 1. åˆå§‹åŒ–å¤„ç†å™¨
    processor = MedicalTextProcessor()
    
    # 2. å®šä¹‰è·¯å¾„ï¼ˆæ ¹æ®ä½ çš„å®é™…è·¯å¾„è°ƒæ•´ï¼‰
    base_path = "D:/lesson/exp4/GraphRAG-Benchmark-main"
    corpus_path = os.path.join(base_path, "Data", "Corpus", "medical.json").replace('\\', '/')
    output_dir = os.path.join(base_path, "processed_data").replace('\\', '/')
    
    # 3. è¿è¡Œå®Œæ•´æµæ°´çº¿
    processed_chunks = processor.process_pipeline(corpus_path, output_dir)
    
    # 4. ç»™å‡ºä¸‹ä¸€æ­¥å»ºè®®
    if processed_chunks:
        print("\nã€ä¸‹ä¸€æ­¥å»ºè®®ã€‘")
        print("1. æ£€æŸ¥ç”Ÿæˆçš„æ–‡æœ¬å—æ–‡ä»¶: processed_data/medical_chunks.json")
        print("2. å‡†å¤‡æ„å»ºå‘é‡æ•°æ®åº“ (Milvus)")
        print("   è¿è¡Œä»¥ä¸‹å‘½ä»¤:")
        print(f"   cd {base_path}")
        print("   python src/vector_store.py  # æˆ‘å°†ä¸ºä½ åˆ›å»ºæ­¤æ–‡ä»¶")
        print("\n3. æµ‹è¯•æ£€ç´¢åŠŸèƒ½")
        print("   ä½¿ç”¨ medical_questions.json ä¸­çš„é—®é¢˜è¿›è¡Œæ£€ç´¢æµ‹è¯•")

if __name__ == "__main__":
    # Windowsæ§åˆ¶å°ç¼–ç è®¾ç½®
    if sys.platform.startswith('win'):
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    
    main()